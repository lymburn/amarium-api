# Armarium API S3 Functions
import typing
from typing import Any, List, Set, Dict, Tuple, Optional, BinaryIO
import boto3
from botocore.exceptions import ClientError
import networkx as nx
from networkx.readwrite import json_graph
import json
import uuid
import base64
import os
import io
from PIL import Image
from storage.s3_connection import s3


# TODO: Instead of printing errors, should raise?


def create_object_key(filename: str) -> str:
    # Generate 6-digit alphanumeric key + append to filename
    # This should allow us to search for filename as a substring of object_key in Files, if needed
    uuid_str = str(uuid.uuid4())[:8]
    obj_key = uuid_str + '-' + filename
    # print(f"DEBUG: Object key, {obj_key}")
    return obj_key


def create_bucket_name() -> str:
    # Use uuid library uuid4() to generate 128-bit seq and take first 12 char
    # NOTE: Using V4 not V1 to ensure higher security, since V1 is generated using MAC addr and timer
    # NOTE: Using uuid b/c it is more random and secure than the random library
    bucket_name = ''
    while True:
        uuid_str = str(uuid.uuid4())[:12]
        bucket_name = 'arm-bucket-' + uuid_str  # 'arm' for Armarium

        if check_bucket_exists(bucket_name) == False:
            break

    return bucket_name


def check_bucket_exists(bucket_name: str) -> bool:
    try:
        exists = True
        s3.meta.client.head_bucket(Bucket=bucket_name)
    except ClientError:
        exists = False
    finally:
        return exists


def create_bucket() -> str:
    bucket_name = create_bucket_name()
    try:
        bucket = s3.create_bucket(
            Bucket=bucket_name,
            CreateBucketConfiguration={
                'LocationConstraint': 'us-east-2'
            }
        )
        bucket.wait_until_exists()
        # print(
        #     f"DEBUG: Created bucket '{bucket.name}' in region={s3.meta.client.meta.region_name}")
    except ClientError as e:
        print(
            f"{e.response['Error']['Code']}: Couldn't create bucket named {bucket_name}.")
    else:
        return bucket_name


def get_buckets() -> List:
    # Returns list of S3 Bucket names, extracted from the returned S3 Bucket objects
    try:
        buckets = list(s3.buckets.all())
        # print(f"DEBUG: Got buckets: {buckets}.")
        bucket_names = [b.name for b in buckets]
    except ClientError as e:
        print(
            f"ClientError, could not get buckets. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        return bucket_names


def delete_bucket(bucket_name: str) -> None:
    # Deletes bucket. Bucket must be empty
    try:
        bucket = s3.Bucket(bucket_name)
        bucket.delete()
        bucket.wait_until_not_exists()
        print(f"Bucket '{bucket.name}'' successfully deleted.")
    except ClientError as e:
        print(
            f"ClientError, could not delete bucket. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


def empty_and_delete_bucket(bucket_name: str) -> None:
    try:
        truncated, objects = list_objects_in_bucket(bucket_name)
        while truncated:
            truncated, objs = list_objects_in_bucket(bucket_name)
            objects.extend(objs)

        object_keys = [obj['Key'] for obj in objects]
        delete_objects(bucket_name, object_keys)

        delete_bucket(bucket_name)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    pass


def list_objects_in_bucket(bucket_name: str) -> Tuple[bool, Any]:
    # Returns up to 1000 items. IsTruncated = true if there are more keys available to return
    try:
        response = s3.meta.client.list_objects_v2(
            Bucket=bucket_name)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        contents = []
        if 'Contents' in response:
            contents = response['Contents']
        return response['IsTruncated'], contents


def upload_data(bytes_buffer: BinaryIO, bucket_name: str, object_key: str) -> None:
    # Given data, upload to given bucket
    # Will overwrite existing object if there is already an obj w same obj_key in bucket
    # NOTE: Assume obj_key is either fetched from DB or generated by calling create_object_key()
    # NOTE: bytes_buffer should be a BytesIO or BufferReader object. Can call getvalue() or read() for bytes
    # NOTE: Stores bytes in bytes_buffer as is. Does not further encode / decode
    try:
        s3.meta.client.upload_fileobj(
            bytes_buffer, bucket_name, object_key)
        # NOTE: May not be necessary to wait
        waiter = s3.meta.client.get_waiter('object_exists')
        waiter.wait(
            Bucket=bucket_name,
            Key=object_key,
            WaiterConfig={
                'Delay': 1,
                'MaxAttempts': 5
            }
        )
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


def upload_image(b64_enc_img: str, bucket_name: str, object_key: str) -> None:
    try:
        # NOTE: Need to decode + re-encode to get a usable byte buffer b/c Swagger passes in str
        # (even if it's an encoded byte string, Python recognizes it as str and not bytes)
        decoded_img = base64.b64decode(b64_enc_img[2:-1]) # Remove b' ' before decoding
        bytes_data = io.BytesIO(base64.b64encode(decoded_img))
        upload_data(bytes_data, bucket_name, object_key)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


def upload_graph(nx_graph, bucket_name: str, graph_name: str) -> None:
    # Translates NetworkX graph obj into JSON, then into BytesIO obj to be saved to S3
    try:
        graph_js = json_graph.node_link_data(nx_graph)
        bytes_data = io.BytesIO(bytes(json.dumps(graph_js).encode('utf-8')))
        upload_data(bytes_data, bucket_name, graph_name)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


def get_graph(bucket_name: str, object_key: str) -> Any:
    # Translate JSON back into NetworkX graph object before returning
    try:
        object_data = get_object(bucket_name, object_key)
        decoded_to_json = json.loads(object_data.decode('utf-8'))
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        return json_graph.node_link_graph(decoded_to_json)


def get_image(object_key: str) -> Image:
    # Decodes S3 returned data, wrap in BytesIO obj, then convert to PIL Image obj
    # NOTE: Assumes returned data be base64 encoded byte stream, b/c that is what is assumed to be uploaded
    # NOTE: Since this returns PIL Image obj, there is no need to call "base64_to_image()" from ML code
    # NOTE: Assumes we only have 1 bucket, so query for buckets + take 1st one
    try:
        buckets = get_buckets()
        img_data = get_object(buckets[0], object_key)
        img_data = base64.b64decode(img_data)
        img_bytes = io.BytesIO(img_data)
        img = Image.open(img_bytes)
        img = img.convert('RGB')  # Get rid of any Alpha color channels
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        return img


def get_image_data(bucket_name: str, object_key: str) -> str:
    try:
        # Get b64 encoded bytes, returned in informal str representation (eg. "b'b64data'")
        img_data = str(get_object(bucket_name, object_key))
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        return img_data
    pass


def delete_objects(bucket_name: str, object_keys: List[str]) -> Tuple[Any, Any]:
    delete_dict = create_delete_objects_delete_dict(object_keys)
    try:
        response = s3.meta.client.delete_objects(
            Bucket=bucket_name, Delete=delete_dict)
        errors = []
        if 'Errors' in response:
            errors = response['Errors']
            for e in errors:
                print(
                    f"Error for key, {e['Key']}. {e['Code']}: {e['Message']}")
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        # List of dicts, with each dict containing 'Key' + other params
        return response['Deleted'], errors


def delete_object(bucket_name: str, object_key: str) -> None:
    # Wrapper for AWS S3 client.delete_object
    try:
        response = s3.meta.client.delete_object(
            Bucket=bucket_name, Key=object_key)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


# AWS S3 Helper Functions
def get_object(bucket_name: str, object_key: str) -> bytes:
    # Wrapper for S3 client.get_object, returns all bytes from Body of response
    # TODO: Consider switching to download_fileobj(BytesIO) if this is slow
    try:
        get_response = s3.meta.client.get_object(
            Bucket=bucket_name, Key=object_key)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        # default read all if no amount specified
        return get_response['Body'].read()


def create_delete_objects_delete_dict(object_keys: List[str]) -> Dict:
    # Given list of object keys, build dict of expected format for client.delete_objects
    objects = []
    for key in object_keys:
        d = {'Key': key}
        objects.append(d)

    return {'Objects': objects}
