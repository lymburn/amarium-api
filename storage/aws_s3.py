# Armarium API S3 Functions
import typing
from typing import Any, List, Set, Dict, Tuple, Optional, BinaryIO
import sqlite3
from sqlite3 import Error
import boto3
from botocore.exceptions import ClientError
import networkx as nx
from networkx.readwrite import json_graph
import json
import uuid
import base64
import os
import io
from PIL import Image


def create_object_key(filename: str) -> str:
    # Generate 6-digit alphanumeric key + append to filename
    # This should allow us to search for filename as a substring of object_key in Files, if needed
    uuid_str = str(uuid.uuid4())[:8]
    obj_key = uuid_str + '/' + filename
    return obj_key


def create_bucket_name(connections) -> str:
    # Use uuid library uuid4() to generate 128-bit seq and take first 12 char
    # NOTE: Using V4 not V1 to ensure higher security, since V1 is generated using MAC addr and timer
    # NOTE: Using uuid b/c it is more random and secure than the random library
    bucket_name = ''
    while True:
        uuid_str = str(uuid.uuid4())[:12]
        bucket_name = 'arm-bucket-' + uuid_str  # 'arm' for Armarium

        if check_bucket_exists(connections, bucket_name) == False:
            break

    return bucket_name


def check_bucket_exists(connections, bucket_name: str) -> bool:
    try:
        connections.s3.meta.client.head_bucket(Bucket=bucket_name)
        # print(f"Bucket '{bucket_name}' exists.")
        exists = True
    except ClientError:
        # print(f"ClientError: Bucket '{bucket_name}' doesn't exist or you don't have access to it.")
        exists = False
    return exists


def create_bucket(connections) -> str:
    bucket_name = create_bucket_name(connections)
    try:
        bucket = connections.s3.create_bucket(
            Bucket=bucket_name,
            CreateBucketConfiguration={
                'LocationConstraint': connections.s3_region
            }
        )
        bucket.wait_until_exists()
        print(
            f"Created bucket '{bucket.name}' in region={connections.s3.meta.client.meta.region_name}")
    except ClientError as e:
        print(
            f"{e.response['Error']['Code']}: Couldn't create bucket named {bucket_name}.")
    else:
        return bucket_name


def get_buckets(connections) -> Any:
    # Returns list of S3 Bucket objects, where name can be accessed by bucket.name
    try:
        buckets = list(connections.s3.buckets.all())
        print(f"Got buckets: {buckets}.")
    except ClientError as e:
        print(
            f"ClientError, could not get buckets. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        return buckets


def delete_bucket(connections, bucket_name: str) -> None:
    # Deletes bucket. Bucket must be empty
    try:
        bucket = connections.s3.Bucket(bucket_name)
        bucket.delete()
        bucket.wait_until_not_exists()
        print(f"Bucket '{bucket.name}'' successfully deleted.")
    except ClientError as e:
        print(
            f"ClientError, could not delete bucket. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


def empty_and_delete_bucket(connections, bucket_name: str) -> None:
    # Checks for objects in bucket, deletes all before deleting bucket
    try:
        # TODO: Get list of object in bucket, delete iteratively, return list of deleted items +/ errors
        delete_bucket(connections, bucket_name)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    pass


def list_objects_in_bucket(connections, bucket_name: str) -> Tuple[bool, Any]:
    # Returns up to 1000 items. IsTruncated = true if there are more keys available to return
    try:
        response = connections.s3.meta.client.list_objects_v2(
            Bucket=bucket_name)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        contents = []
        if 'Contents' in response:
            contents = response['Contents']
        return response['IsTruncated'], contents


def upload_data(connections, bytes_buffer: BinaryIO, bucket_name: str, object_key: str) -> None:
    # Given data, upload to given bucket
    # Will overwrite existing object if there is already an obj w same obj_key in bucket
    # NOTE: Assume obj_key is either fetched from DB or generated by calling create_object_key()
    # NOTE: bytes_buffer should be a BytesIO or BufferReader object. Can call getvalue() or read() for bytes
    # NOTE: Stores bytes in bytes_buffer as is. Does not further encode / decode
    try:
        connections.s3.meta.client.upload_fileobj(
            bytes_buffer, bucket_name, object_key)
        # NOTE: May not be necessary to wait
        waiter = connections.s3.meta.client.get_waiter('object_exists')
        waiter.wait(
            Bucket=bucket_name,
            Key=object_key,
            WaiterConfig={
                'Delay': 1,
                'MaxAttempts': 5
            }
        )
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


def upload_graph(connections, nx_graph, bucket_name: str, graph_name: str) -> None:
    # Translates NetworkX graph obj into JSON, then into BytesIO obj to be saved to S3
    try:
        graph_js = json_graph.node_link_data(nx_graph)
        bytes_data = io.BytesIO(bytes(json.dumps(graph_js).encode('utf-8')))
        upload_data(connections, bytes_data, bucket_name, graph_name)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


def get_graph(connections, bucket_name: str, object_key: str) -> Any:
    # Translate JSON back into NetworkX graph object before returning
    try:
        object_data = get_object(connections, bucket_name, object_key)
        decoded_to_json = json.loads(object_data.decode('utf-8'))
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        return json_graph.node_link_graph(decoded_to_json)


def get_image(connections, bucket_name: str, object_key: str) -> Image:
    # Decodes S3 returned data, wrap in BytesIO obj, then convert to PIL Image obj
    # NOTE: Assumes returned data be base64 encoded byte stream, b/c that is what is assumed to be uploaded
    # NOTE: Since this returns PIL Image obj, there is no need to call "base64_to_image()" from ML code
    try:
        img_data = get_object(connections, bucket_name, object_key)
        img_data = base64.b64decode(img_data)
        img_bytes = io.BytesIO(img_data)
        img = Image.open(img_bytes)
        img = img.convert('RGB')  # Get rid of any Alpha color channels
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        return img


def delete_objects(connections, bucket_name: str, object_keys: List[str]) -> Tuple[Any, Any]:
    delete_dict = create_delete_objects_delete_dict(object_keys)
    try:
        response = connections.s3.meta.client.delete_objects(
            Bucket=bucket_name, Delete=delete_dict)
        errors = response['Errors']
        if errors:
            for e in errors:
                print(
                    f"Error for key, {e['Key']}. {e['Code']}: {e['Message']}")
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")
    else:
        # List of dicts, with each dict containing 'Key' + other params
        return response['Deleted'], errors


def delete_object(connections, bucket_name: str, object_key: str) -> None:
    # Wrapper for AWS S3 client.delete_object
    try:
        response = connections.s3.meta.client.delete_object(
            Bucket=bucket_name, Key=object_key)
    except ClientError as e:
        print(
            f"ClientError exception. {e.response['Error']['Code']}: {e.response['Error']['Message']}")


# AWS S3 Helper Functions
def get_object(connections, bucket_name: str, object_key: str):
    # Wrapper for S3 client.get_object, returns all bytes from Body of response
    # TODO: Consider switching to download_fileobj(BytesIO) if this is slow
    get_response = connections.s3.meta.client.get_object(
        Bucket=bucket_name, Key=object_key)
    # default read all if no amount specified
    return get_response['Body'].read()


def create_delete_objects_delete_dict(object_keys: List[str]):
    # Given list of object keys, build dict of expected format for client.delete_objects
    objects = []
    for key in object_keys:
        d = {'Key': key}
        objects.append(d)

    return {'Objects': objects}
